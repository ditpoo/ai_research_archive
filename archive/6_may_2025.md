https://x.com/ZeyuanAllenZhu/status/1918684257058197922

https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5240330


10B parameter diffusion model , trained exclusively on copyright-safe and SFW content. 

https://huggingface.co/Freepik/F-Lite

web ssl, scalable clip alternative for vllm's from meta

https://huggingface.co/collections/facebook/web-ssl-68094132c15fbd7808d1e9bb


Taming the Titans: A Survey of Efficient LLM Inference Serving

https://arxiv.org/abs/2504.19720

Reinforcement Learning: A Comprehensive Overview

https://arxiv.org/abs/2412.05265v2

BitNet v2: Native 4-bit Activations with Hadamard Transformation for 1-bit LLMs

https://arxiv.org/abs/2504.18415

Softpick: No Attention Sink, No Massive Activations

https://arxiv.org/abs/2504.20966

Generative AI for Character Animation: A Comprehensive Survey of Techniques, Applications, and Future Directions
Survey of Character Animation

https://arxiv.org/abs/2504.19056

Inductive Moment Matching (diffusion in less steps)

https://arxiv.org/abs/2503.07565

GaLore 2: Large-Scale LLM Pre-Training by Gradient Low-Rank Projection

https://www.arxiv.org/abs/2504.20437


MAGI-1: Autoregressive Video Generation at Scale 
24b, 4.5b distilled model fp8 quant

https://github.com/sandai-org/MAGI-1


Matrix-vector multiplication implemented in off-the-shelf DRAM for Low-Bit LLMs
matrix compute in ram

https://news.ycombinator.com/item?id=43890538

Graceful Shutdown in Go: Practical Patterns

https://news.ycombinator.com/item?id=43889610


Show HN: VectorVFS, your filesystem as a vector database
using metadata for files

https://news.ycombinator.com/item?id=43896011


Linkwarden: FOSS self-hostable bookmarking with AI-tagging and page archival

https://news.ycombinator.com/item?id=43856801

Show HN: Real-time AI Voice Chat at ~500ms Latency 

https://news.ycombinator.com/item?id=43899028

RealtimeVoiceChat

https://github.com/KoljaB/RealtimeVoiceChat

Inference needs nontrivial amount of PCIe bandwidth (8x RTX 3090 rig, tensor parallelism) 
(tensor parallelism vs pipeline parallesim wrt to pcie bandwidth)

https://www.reddit.com/r/LocalLLaMA/comments/1kds51e/inference_needs_nontrivial_amount_of_pcie/

Dia-JAX – Run a 1.6B Text-to-Speech Model on TPU with JAX

https://www.reddit.com/r/LocalLLaMA/comments/1kdre6g/diajax_run_a_16b_texttospeech_model_on_tpu_with/

Dia is a 1.6B parameter text to speech model created by Nari Labs.

https://github.com/nari-labs/dia/tree/main

https://huggingface.co/nari-labs/Dia-1.6B

LLMs use tools with RL! trained 0.5B/3B Qwen models to use a calculator tool

https://www.reddit.com/r/LocalLLaMA/comments/1kdqcjk/teaching_llms_to_use_tools_with_rl_successfully/

Nvidia ASR stt 600m param model current fastest

https://huggingface.co/nvidia/parakeet-tdt-0.6b-v2

Granite-4-Tiny-Preview is a 7B A1 MoE, 1b active mamab, attention hybrid MOE

https://www.reddit.com/r/LocalLLaMA/comments/1kd38c7/granite4tinypreview_is_a_7b_a1_moe/

https://huggingface.co/ibm-granite/granite-4.0-tiny-preview

VRAM Requirements Reference - What can you run with your VRAM? (Contributions welcome) 

https://www.reddit.com/r/LocalLLaMA/comments/1kank02/vram_requirements_reference_what_can_you_run_with/

Qwen3 performance benchmarks (toks/s, RAM utilization, etc.) on ~50 devices (iOS, Android, Mac, Windows) unsloth

https://www.reddit.com/r/LocalLLaMA/comments/1kepuli/qwen3_performance_benchmarks_tokss_ram/

XTTS v1 — Techincal Notes

https://medium.com/@erogol/xtts-v1-techincal-notes-eb83ff05bdc

Optimizing XTTS-v2: Vocalize the first Harry Potter book in 10 minutes & ~10GB VRAM 

https://www.reddit.com/r/LocalLLaMA/comments/1h3b4sg/optimizing_xttsv2_vocalize_the_first_harry_potter/

https://huggingface.co/AstraMindAI/xttsv2

Audio duplex models 

kyutai labs moshi model,  speech-text foundation model and full-duplex spoken dialogue framework

https://github.com/kyutai-labs/moshi

https://arxiv.org/abs/2410.00037

Hertz-dev is an open-source, first-of-its-kind base model for full-duplex conversational audio.

https://huggingface.co/si-pbc/hertz-dev

Retrieval based Voice Conversion (RVC) 

https://huggingface.co/ArkanDash/rvc-genshin-impact

https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI

Fish Agent V0.1 3B Voice-to-Voice model capable of capturing and generating environmental audio

https://huggingface.co/fishaudio/fish-agent-v0.1-3b

fish-speech end-to-end TTS 1.5gb

https://github.com/fishaudio/fish-speech

https://huggingface.co/fishaudio/fish-speech-1.5

Muchi 8B duplex tts based on mushi 

Muchi is a finetuned speech-text foundation model and full-duplex 

https://huggingface.co/DavidBrowne17/Muchi

Language Model Can Listen While Speaking

we explore full duplex modeling (FDM) in interactive speech language models (iSLM), focusing on enhancing real-time interaction and, more explicitly, exploring the quintessential ability of interruption. We introduce a novel model design, namely listening-while-speaking language model (LSLM), an end-to-end system equipped with both listening and speaking channels.

https://huggingface.co/papers/2408.02622

https://arxiv.org/abs/2408.02622


x links

gui for llms/ai

https://x.com/karpathy/status/1917920257257459899

Monthly overview of model released 

https://x.com/AdinaYakup/status/1917621106573275288

Meta Reasoner-8b retriever (re-rankers)

https://x.com/rohanpaul_ai/status/1917649963422458289

Apple Tara flow (Normalizing flow diffusion models)

https://x.com/zhaisf/status/1918544263609983222

AI agent protocols survey

https://x.com/omarsar0/status/1918723145923453022

Canon layers 

https://x.com/ZeyuanAllenZhu/status/1918684257058197922

Tiny mixtral MoE 172m params 8 experts

https://x.com/kabir_j25/status/1918938850149138934

Yarn-Mistral-7b-128k

https://x.com/theemozilla/status/1720107186850877662

List of llm benchmarks

https://x.com/scaling01/status/1919092778648408363

late chunking

https://x.com/doesdatmaksense/status/1919034855851254236

parallel formulation of attention for 15% train speedup withou any tradeoff

https://x.com/gabriberton/status/1919221523476320257

100 days after deepseek R1 survey

https://x.com/_philschmid/status/1918898257406709983

canon layers might improve training stability

https://x.com/_arohan_/status/1919104875843158110

Nvidia llama nemotron reasoning models faimly

https://x.com/arankomatsuzaki/status/1919236158351147087

https://x.com/_akhaliq/status/1919324939934453928

we’re training for heuristics now. Reasoning capacities are drawn from examples the pretraining data but it’s all mixed signal and SOTA synth pipelines will take the shortcut.

https://x.com/Dorialexander/status/1918961019000344912

ConceptAttention creates rich saliency maps of text concepts present in generated images and videos ICML 25

https://x.com/alec_helbling/status/1919377957371429045

param delta, weights delta between post-trained and pre-trained checkpoints can be directly applied to related models without any training. 

https://x.com/tydsh/status/1919451792452288542

D-FINE (@iclr_conf '25 spotlight) was just added. SOTA real-time object detection

https://x.com/NielsRogge/status/1919381777291870645

dspy grpo

https://x.com/lateinteraction/status/1919428454761553994

Inductive Moment Matching one step sampler

https://x.com/s_scardapane/status/1919406139147538538

Keras RS recommender library

https://x.com/fchollet/status/1919477586599805118

Magi 4.5b distilled from 24b video gen model

https://x.com/SandAI_HQ/status/1919406732322754934


expert parallelism and prefill-decode disaggregation on 96 GPUs.

This optimized strategy improves output throughput by up to 5x compared to vanilla tensor parallelism.

https://x.com/ChujieZheng/status/1919448724251525167

llm inference engine from scratch in c++

https://x.com/_apoorvnandan/status/1919411513045230048

rl / grpo to masked diffusion llm's (dllm's)

https://x.com/siyan_zhao/status/1919426158589775891

titok recommender sys without n/w / social graph

https://x.com/LiorOnAI/status/1919427492911182013

p2p with nvidia 5000 series gpu (direct gpu 2 gpu data communication) bandthwidth boost

https://x.com/lauriewired/status/1919559866537882076

LlamaGym: a tiny library to fine-tune LLM agents with online RL.

https://x.com/khoomeik/status/1919549712375677195

https://github.com/KhoomeiK/LlamaGym

rl starter open ai

https://x.com/curiousZeedX/status/1919386210243358859

Universal RAG across modalities

https://x.com/omarsar0/status/1917637837295608180

Deep seek prover v2

https://x.com/zhs05232838/status/1917600755936018715

Kimi-Audio! Our new open-source audio foundation model advances capabilities in audio understanding, generation, and conversation.
ASR, TTS (based on qwen 2.5 7b)

https://x.com/Kimi_Moonshot/status/1915807071960007115

xiaomi MiMo-7b Reasoning

https://x.com/omarsar0/status/1917582720341008814

Advances and Challenges in Foundation Agents, report

https://x.com/dair_ai/status/1919017842860548399

MAGI is a new multi-agent system that dynamically navigates clinical logic via four specialized agents. (psychiatric assesment)

https://x.com/omarsar0/status/1916862752410554423

Chain of Draft

https://x.com/LiorOnAI/status/1919044880631775332

Which attention heads matter at scale for ICL

https://x.com/kayo_yin/status/1895259577316475381

Pixel Hacker Image inpainting

https://x.com/HuggingPapers/status/1919306031705866518

T2I - R1, rl based image generation

https://x.com/_akhaliq/status/1918956338442309948

Princeton and Meta AI just released COMPACT on Hugging Face

A new data recipe that scales capabilities of Multimodal LLMs by explicitly controlling the compositional complexity of the training examples!

https://x.com/HuggingPapers/status/1918638512921809006

